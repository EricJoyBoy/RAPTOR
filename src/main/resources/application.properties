# Ollama Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3.1:8b
spring.ai.ollama.embedding.model=nomic-embed-text

# Chat options
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.top-p=0.9
spring.ai.ollama.chat.options.num-predict=2048

# RAPTOR Configuration
raptor.processing.default-chunk-size=2000
raptor.processing.default-max-levels=3
raptor.processing.max-text-length=1000000
raptor.processing.max-file-size-mb=10
raptor.processing.enable-async-processing=false
raptor.processing.enable-caching=true

raptor.clustering.cluster-threshold=0.1
raptor.clustering.max-clusters=50
raptor.clustering.max-iterations=100
raptor.clustering.local-max-iterations=50
raptor.clustering.seed=224

raptor.security.enable-rate-limiting=false
raptor.security.max-requests-per-minute=100
raptor.security.enable-authentication=false

raptor.monitoring.enable-metrics=true
raptor.monitoring.enable-tracing=false
raptor.monitoring.enable-health-checks=true

# Spring Boot Actuator
management.endpoints.web.exposure.include=health,info,metrics
management.endpoint.health.show-details=always
management.health.defaults.enabled=true

# Logging
logging.level.it.raptor_service=DEBUG
logging.level.org.springframework.ai=DEBUG
logging.level.org.springframework.boot.actuate.health=INFO

# Server configuration
server.port=8080
server.error.include-message=always
server.error.include-binding-errors=always