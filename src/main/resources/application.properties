# Ollama Configuration
spring.ai.ollama.base-url=http://localhost:11434
spring.ai.ollama.chat.model=llama3.1:8b
spring.ai.ollama.embedding.model=nomic-embed-text

# Chat options
spring.ai.ollama.chat.options.temperature=0.7
spring.ai.ollama.chat.options.top-p=0.9
spring.ai.ollama.chat.options.num-predict=2048

# Logging
logging.level.it.raptor_service=DEBUG
logging.level.org.springframework.ai=DEBUG